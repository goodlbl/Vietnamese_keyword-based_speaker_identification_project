{% load static %}
<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Wake-Word Detector</title>
    <!-- 1. T·∫£i Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- 2. T·∫£i ONNX Runtime (ƒë·ªÉ ch·∫°y model) -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.18.0/dist/ort.min.js"></script>
    
    <!-- 3. T·∫£i TensorFlow.js (ch·ªâ ƒë·ªÉ ti·ªÅn x·ª≠ l√Ω STFT) -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.20.0/dist/tf.min.js"></script>
    
    <style>
        /* Th√™m style cho n√∫t b·∫•m disabled */
        #toggle-btn:disabled {
            background-color: #9ca3af; /* gray-400 */
            cursor: not-allowed;
        }
    </style>
</head>
<body class="bg-gray-100 font-sans antialiased">
    <div class="container mx-auto p-4 sm:p-8 max-w-2xl">
        <div class="bg-white rounded-lg shadow-xl overflow-hidden">
            <div class="p-6 sm:p-8">
                <h1 class="text-2xl sm:text-3xl font-bold text-center text-gray-800 mb-6">
                    Real-time Wake Word Detector
                </h1>
                
                <p class="text-center text-gray-600 mb-6">
                    Nh·∫•n "B·∫Øt ƒë·∫ßu" v√† th·ª≠ n√≥i t·ª´ kh√≥a.
                    Model ƒëang nghe v√† ph√¢n t√≠ch li√™n t·ª•c trong tr√¨nh duy·ªát.
                </p>

                <!-- V√πng ƒëi·ªÅu khi·ªÉn -->
                <div class="flex justify-center mb-6">
                    <button id="toggle-btn"
                            class="bg-blue-500 hover:bg-blue-600 text-white font-semibold py-3 px-6 rounded-lg text-lg transition duration-300"
                            onclick="toggleListening()">
                        B·∫Øt ƒë·∫ßu nghe
                    </button>
                </div>

                <!-- V√πng hi·ªÉn th·ªã tr·∫°ng th√°i/k·∫øt qu·∫£ -->
                <div id="status" class="mt-6 text-center p-4 rounded-lg bg-gray-50 hidden"></div>
                <div id="features-box" class="mt-6 text-center"></div>
                <!-- ‚úÖ N√∫t ƒêƒÉng k√Ω -->
<form method="get" action="{% url 'check_password:check_password_view' room.id %}" class="flex justify-center mt-8">
    <button type="submit"
        class="bg-gradient-to-r from-green-400 to-blue-500 hover:from-green-500 hover:to-blue-600
               text-white font-semibold py-3 px-8 rounded-full text-lg shadow-md
               transform hover:scale-105 transition duration-300 ease-in-out">
        üìù ƒêƒÉng k√Ω
    </button>
</form>
            </div>
        </div>
    </div>

    <script>
        // --- C√ÄI ƒê·∫∂T C·∫§U H√åNH (T·ª™ FILE C≈® C·ª¶A B·∫†N) ---
        
        // 1. ƒê∆∞·ªùng d·∫´n model (ƒë√£ s·ª≠a)
        const ONNX_MODEL_PATH = "{% static 'model/voice_model_from_spec.onnx' %}";
        //  L·∫•y th√¥ng tin ph√≤ng t·ª´ Django context
        const ROOM_ID = "{{ room.id }}";
        // 2. C√°c tham s·ªë x·ª≠ l√Ω (ƒë√£ s·ª≠a)
        const TARGET_SR = 16000;
        const TARGET_LENGTH = 40000; // 2.5 gi√¢y * 16000
        const N_FFT = 400;           // Ph·∫£i kh·ªõp v·ªõi model (201 bins)
        const HOP_LENGTH = 512;
        
        // 3. Ng∆∞·ª°ng (Threshold)
        const THRESHOLD = 0.5; // B·∫°n c√≥ th·ªÉ ch·ªânh gi√° tr·ªã n√†y (vd: 0.8)

        // 4. Kho·∫£ng th·ªùi gian gi·ªØa 2 l·∫ßn d·ª± ƒëo√°n (t√≠nh b·∫±ng mili-gi√¢y)
        // S·ªë c√†ng nh·ªè, model ch·∫°y c√†ng nhi·ªÅu, c√†ng t·ªën CPU
        const INFERENCE_INTERVAL_MS = 750; // Ch·∫°y d·ª± ƒëo√°n m·ªói 0.75 gi√¢y

        // --- Bi·∫øn to√†n c·ª•c (Global Variables) ---
        let ortSession;
        let isListening = false;
        let audioContext;
        let micStream;
        let scriptProcessor;
        let audioBuffer = new Float32Array(0); // B·ªô ƒë·ªám √¢m thanh th√¥ (·ªü SR g·ªëc)
        let inferenceInterval;

        const statusDiv = document.getElementById('status');
        const toggleBtn = document.getElementById('toggle-btn');
        const featuresBox = document.getElementById('features-box');


        // =================================================================
        // C√ÅC H√ÄM C·ªêT L√ïI (GI·ªÆ NGUY√äN T·ª™ FILE C≈® C·ª¶A B·∫†N)
        // =================================================================

        // 2. Resample (GI·ªÆ NGUY√äN)
        async function resampleAudio(audioBuffer, targetSampleRate) {
            if (audioBuffer.sampleRate === targetSampleRate) {
                return audioBuffer.getChannelData(0);
            }
            const duration = audioBuffer.duration;
            const offlineContext = new OfflineAudioContext(
                1, Math.ceil(duration * targetSampleRate), targetSampleRate
            );
            const bufferSource = offlineContext.createBufferSource();
            bufferSource.buffer = audioBuffer;
            bufferSource.connect(offlineContext.destination);
            bufferSource.start();
            const resampledBuffer = await offlineContext.startRendering();
            return resampledBuffer.getChannelData(0);
        }

        // 3. Pad/Cut (GI·ªÆ NGUY√äN)
        function padOrCut(data, length) {
            if (data.length > length) {
                return data.slice(0, length);
            } else if (data.length < length) {
                const paddedData = new Float32Array(length).fill(0);
                paddedData.set(data);
                return paddedData;
            }
            return data;
        }

        // 4. T√≠nh Spectrogram (GI·ªÆ NGUY√äN V√Ä ƒê√É S·ª¨A CHO N_FFT=400)
        async function computeSpectrogram(data) {
            const padWidth = N_FFT / 2; // 400 / 2 = 200
            const inputTensor = tf.tensor1d(data);
            // 1. Reflect padding [[200, 200]]
            const paddedTensor = tf.mirrorPad(inputTensor, [[padWidth, padWidth]], 'reflect');
            // 2. STFT -> [79, 201]
            const stft = tf.signal.stft(
                paddedTensor, N_FFT, HOP_LENGTH, N_FFT, tf.signal.hannWindow
            );
            // 3. Power Spectrogram
            const powerSpec = tf.square(tf.abs(stft));
            // 4. Transpose -> [201, 79]
            const transposedSpec = powerSpec.transpose([1, 0]);

            // D·ªçn d·∫πp
            inputTensor.dispose();
            paddedTensor.dispose();
            stft.dispose();
            powerSpec.dispose();
            
            return transposedSpec; // Tr·∫£ v·ªÅ tensor [201, 79]
        }

        // 5. Ch·∫°y Model (GI·ªÆ NGUY√äN)
        async function runOnnxModel(spectrogramTensor) {
            const specData = await spectrogramTensor.data();
            const dims = spectrogramTensor.shape; // [201, 79]
            // Th√™m chi·ªÅu batch [1, 201, 79]
            const ortTensor = new ort.Tensor('float32', specData, [1, ...dims]);
            const inputs = { 'spectrogram': ortTensor };
            const results = await ortSession.run(inputs);
            const logit = results.logits.data[0];
            const probability = 1 / (1 + Math.exp(-logit));
            const prediction = probability > THRESHOLD ? "True" : "False";
            spectrogramTensor.dispose();
            return [prediction, probability];
        }

        // 6. C·∫≠p nh·∫≠t UI (GI·ªÆ NGUY√äN)
        function updateStatus(message, type) {
             statusDiv.classList.remove('hidden', 'bg-gray-50', 'bg-blue-100', 'bg-green-100', 'bg-red-100', 'text-gray-800', 'text-blue-800', 'text-green-800', 'text-red-800');
            let baseClass = 'p-4 rounded-lg';
            let typeClass = '';
            if (type === 'loading') {
                typeClass = 'bg-blue-100 text-blue-800';
                message = `<div class="flex items-center justify-center"><svg class="animate-spin -ml-1 mr-3 h-5 w-5" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg> <span>${message}</span></div>`;
            } else if (type === 'success') {
                typeClass = 'bg-green-100 text-green-800';
            } else if (type === 'error') {
                typeClass = 'bg-red-100 text-red-800';
            } else {
                typeClass = 'bg-gray-50 text-gray-800';
            }
            statusDiv.className = `${baseClass} ${typeClass}`;
            statusDiv.innerHTML = message;
        }

        // =================================================================
        // LOGIC M·ªöI (REAL-TIME)
        // =================================================================

        // 0. T·∫£i model khi trang m·ªü
        async function loadModel() {
            toggleBtn.disabled = true;
            try {
                updateStatus("ƒêang t·∫£i model ONNX...", "loading");
                ortSession = await ort.InferenceSession.create(ONNX_MODEL_PATH);
                updateStatus("Model ƒë√£ s·∫µn s√†ng. Nh·∫•n 'B·∫Øt ƒë·∫ßu nghe' ƒë·ªÉ ch·∫°y.", "success");
                toggleBtn.disabled = false;
            } catch (e) {
                updateStatus(`L·ªói khi t·∫£i model: ${e.message}`, "error");
            }
        }
        
        // 1. H√†m ch√≠nh ƒëi·ªÅu khi·ªÉn B·∫Øt ƒë·∫ßu / D·ª´ng
        function toggleListening() {
            if (isListening) {
                stopListening();
            } else {
                startListening();
            }
        }

        // 2. B·∫Øt ƒë·∫ßu nghe
        async function startListening() {
            if (!ortSession) {
                updateStatus("L·ªói: Model ch∆∞a ƒë∆∞·ª£c t·∫£i.", "error");
                return;
            }

            try {
                updateStatus("Y√™u c·∫ßu quy·ªÅn truy c·∫≠p micro...", "loading");
                // 1. T·∫°o AudioContext v√† l·∫•y stream
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                // 2. T·∫°o ScriptProcessor ƒë·ªÉ nh·∫≠n data
                // bufferSize 4096, 1 k√™nh v√†o, 1 k√™nh ra
                scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                
                // 3. ƒêƒÉng k√Ω callback
                // H√†m n√†y s·∫Ω ch·∫°y m·ªói khi buffer 4096-sample ƒë·∫ßy
                scriptProcessor.onaudioprocess = processAudioChunk;
                
                // 4. K·∫øt n·ªëi c√°c node
                const source = audioContext.createMediaStreamSource(micStream);
                source.connect(scriptProcessor);
                scriptProcessor.connect(audioContext.destination); // Ph·∫£i k·∫øt n·ªëi ƒë·ªÉ n√≥ ch·∫°y

                // 5. B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p d·ª± ƒëo√°n
                // C·ª© m·ªói 750ms, ch√∫ng ta s·∫Ω ch·∫°y h√†m runInference
                inferenceInterval = setInterval(runInference, INFERENCE_INTERVAL_MS);

                isListening = true;
                toggleBtn.textContent = "D·ª´ng nghe";
                toggleBtn.classList.replace('bg-blue-500', 'bg-red-500');
                toggleBtn.classList.replace('hover:bg-blue-600', 'hover:bg-red-600');
                updateStatus("ƒêang nghe...", "loading");

            } catch (e) {
                updateStatus(`L·ªói micro: ${e.message}. Vui l√≤ng c·∫•p quy·ªÅn truy c·∫≠p.`, "error");
            }
        }

        // 3. D·ª´ng nghe
        function stopListening() {
            if (!isListening) return;

            try {
                // D·ª´ng v√≤ng l·∫∑p
                clearInterval(inferenceInterval);
                inferenceInterval = null;

                // D·ª´ng c√°c node audio
                scriptProcessor.disconnect();
                micStream.getTracks().forEach(track => track.stop());
                audioContext.close();

                // Reset tr·∫°ng th√°i
                isListening = false;
                audioBuffer = new Float32Array(0); // X√≥a buffer
                toggleBtn.textContent = "B·∫Øt ƒë·∫ßu nghe";
                toggleBtn.classList.replace('bg-red-500', 'bg-blue-500');
                toggleBtn.classList.replace('hover:bg-red-600', 'hover:bg-blue-600');
                updateStatus("ƒê√£ d·ª´ng. Nh·∫•n 'B·∫Øt ƒë·∫ßu' ƒë·ªÉ nghe l·∫°i.", "success");
            } catch (e) {
                console.error("L·ªói khi d·ª´ng:", e);
                updateStatus("ƒê√£ d·ª´ng.", "success");
            }
        }

        // 4. Callback x·ª≠ l√Ω t·ª´ng kh·ªëi √¢m thanh (Sync)
        // H√†m n√†y ch·ªâ l√†m 1 vi·ªác: ƒë∆∞a data v√†o buffer
        function processAudioChunk(event) {
            if (!isListening) return;
            
            // L·∫•y data √¢m thanh (·ªü SR g·ªëc c·ªßa tr√¨nh duy·ªát, vd: 48000Hz)
            const newChunk = event.inputBuffer.getChannelData(0);
            
            // Th√™m v√†o buffer
            const newBuffer = new Float32Array(audioBuffer.length + newChunk.length);
            newBuffer.set(audioBuffer);
            newBuffer.set(newChunk, audioBuffer.length);
            audioBuffer = newBuffer;

            // Gi·ªõi h·∫°n buffer, ch·ªâ gi·ªØ 5 gi√¢y cu·ªëi ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ
            const maxBufferLength = (audioContext.sampleRate * 5); 
            if (audioBuffer.length > maxBufferLength) {
                audioBuffer = audioBuffer.slice(audioBuffer.length - maxBufferLength);
            }
        }

        // 5. H√†m ch·∫°y d·ª± ƒëo√°n (Async)
        async function runInference() {
            if (!isListening) return;

            // T√≠nh to√°n s·ªë m·∫´u ·ªü SR g·ªëc t∆∞∆°ng ƒë∆∞∆°ng v·ªõi 2.5s (40000 m·∫´u ·ªü 16kHz)
            const requiredNativeSamples = Math.floor(TARGET_LENGTH * (audioContext.sampleRate / TARGET_SR));

            // N·∫øu buffer ch∆∞a ƒë·ªß 2.5s, ch·ªù
            if (audioBuffer.length < requiredNativeSamples) {
                updateStatus("ƒêang nghe (ch·ªù ƒë·ªß buffer)...", "loading");
                return;
            }

            // L·∫•y 2.5s √¢m thanh cu·ªëi c√πng t·ª´ buffer
            const audioChunk = audioBuffer.slice(audioBuffer.length - requiredNativeSamples);

            // T·∫°m th·ªùi v√¥ hi·ªáu h√≥a n√∫t b·∫•m trong khi x·ª≠ l√Ω
            toggleBtn.disabled = true;
            updateStatus("ƒêang ph√¢n t√≠ch...", "loading");

            try {
                // 1. T·∫°o AudioBuffer object ƒë·ªÉ d√πng h√†m resample
                const bufferToResample = audioContext.createBuffer(1, audioChunk.length, audioContext.sampleRate);
                bufferToResample.copyToChannel(audioChunk, 0);

                // 2. Resample (Async)
                const resampledData = await resampleAudio(bufferToResample, TARGET_SR);

                // 3. Pad/Cut (Async - d√π h√†m l√† sync)
                const processedData = padOrCut(resampledData, TARGET_LENGTH);

                // 4. T√≠nh Spectrogram (Async)
                const spectrogramTensor = await computeSpectrogram(processedData);

                // 5. Ch·∫°y Model (Async)
                const [prediction, probability] = await runOnnxModel(spectrogramTensor);

                // 6. Ki·ªÉm tra k·∫øt qu·∫£
                if (prediction === "True" && probability > THRESHOLD) {
                    updateStatus(
                        `<strong>Ph√°t hi·ªán t·ª´ kh√≥a!</strong><br>
                        X√°c su·∫•t: ${(probability * 100).toFixed(2)}%<br>
                        üîÑ ƒêang g·ª≠i t·ªõi Model2 (x√°c th·ª±c ng∆∞·ªùi n√≥i)...`,
                        "loading"
                    );

                    // D·ª´ng l·∫Øng nghe ƒë·ªÉ x·ª≠ l√Ω
                    stopListening();

                    try {
                        // üëâ 1. Chuy·ªÉn audio hi·ªán t·∫°i th√†nh WAV Blob
                        const wavBlob = await exportWavBlob(audioChunk, audioContext.sampleRate);

                        // üëâ 2. G·ª≠i th·∫≥ng t·ªõi Django
                        const formData = new FormData();
                        formData.append("audio", wavBlob, "captured.wav");
                        formData.append("room_id", ROOM_ID);

                        const response = await fetch("/action_room/verify_voice/", {
                            method: "POST",
                            body: formData
                        });

                        const data = await response.json();

                        // üëâ 3. Hi·ªÉn th·ªã k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ Model2
                        if (data.error) {
                            updateStatus(`‚ùå L·ªói: ${data.error}`, "error");
                        } else {
                            updateStatus(
                                `üé§ Gi·ªçng ng∆∞·ªùi n√≥i: <strong>${data.owner}</strong><br>
                                üîπ Similarity: <strong>${(data.similarity * 100).toFixed(2)}%</strong><br>
                                üîπ K·∫øt qu·∫£: <strong>${data.is_match ? "‚úÖ Kh·ªõp" : "‚ùå Kh√¥ng kh·ªõp"}</strong>`,
                                data.is_match ? "success" : "error"
                            );
                                if (data.is_match) {
                                    // ‚úÖ N·∫øu gi·ªçng kh·ªõp ‚Üí hi·ªán ch·ª©c nƒÉng
                                    if (data.functions && data.functions.length > 0) {
                                        const colors = ["bg-red-500", "bg-green-500", "bg-blue-500", "bg-pink-500", "bg-orange-500", "bg-teal-500"];
                                        const html = data.functions.map((f, i) =>
                                            `<button class="${colors[i % colors.length]} text-white px-4 py-2 rounded-lg m-2 hover:opacity-90">${f}</button>`
                                        ).join("");
                                        featuresBox.innerHTML = `
                                            <div class="mt-4 bg-white p-4 rounded-xl shadow-md">
                                                <h3 class="text-lg font-bold mb-2">Thi·∫øt b·ªã c·ªßa ${data.owner}</h3>
                                                ${html}
                                            </div>
                                        `;
                                    } else {
                                        featuresBox.innerHTML = `<p class="text-gray-500 mt-4 italic">Kh√¥ng c√≥ thi·∫øt b·ªã n√†o ƒë∆∞·ª£c ph√¢n quy·ªÅn.</p>`;
                                    }
                                } else {
                                    featuresBox.innerHTML = "";
                                }
                            }
                            toggleBtn.disabled = false;
                            toggleBtn.textContent = "B·∫Øt ƒë·∫ßu nghe";
                            toggleBtn.classList.remove('bg-red-500', 'hover:bg-red-600');
                            toggleBtn.classList.add('bg-blue-500', 'hover:bg-blue-600');
                            // üß© Th√™m th√¥ng b√°o ph·ª• m√† kh√¥ng x√≥a k·∫øt qu·∫£ c≈©
                            statusDiv.innerHTML += `<div class="mt-2 text-green-700 font-medium">
                                Ho√†n t·∫•t. B·∫°n c√≥ th·ªÉ b·∫•m <strong>'B·∫Øt ƒë·∫ßu nghe'</strong> ƒë·ªÉ ti·∫øp t·ª•c.
                            </div>`;
                        

                    } catch (err) {
                        console.error("L·ªói khi g·ª≠i Flask:", err);
                        updateStatus(`üî• L·ªói khi x√°c th·ª±c gi·ªçng n√≥i: ${err.message}`, "error");
                    }
                } else {
                    // V·∫´n ƒëang nghe
                    updateStatus(
                        `ƒêang nghe... (G·∫ßn nh·∫•t: ${(probability * 100).toFixed(2)}%)`,
                        "loading"
                    );
                    toggleBtn.disabled = false; // B·∫≠t l·∫°i n√∫t
                }
            } catch (e) {
                updateStatus(`L·ªói khi d·ª± ƒëo√°n: ${e.message}`, "error");
                toggleBtn.disabled = false;
            }
        }


        // üîß Chuy·ªÉn m·∫£ng Float32Array th√†nh file WAV (16-bit PCM)
        async function exportWavBlob(float32Array, sampleRate) {
            // Chuy·ªÉn float32 ‚Üí int16
            const buffer = new ArrayBuffer(44 + float32Array.length * 2);
            const view = new DataView(buffer);

            // WAV header
            const writeString = (offset, str) => {
                for (let i = 0; i < str.length; i++) view.setUint8(offset + i, str.charCodeAt(i));
            };
            writeString(0, "RIFF");
            view.setUint32(4, 36 + float32Array.length * 2, true);
            writeString(8, "WAVE");
            writeString(12, "fmt ");
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true); // PCM
            view.setUint16(22, 1, true); // mono
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true); // byte rate
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(36, "data");
            view.setUint32(40, float32Array.length * 2, true);

            // PCM samples
            let offset = 44;
            for (let i = 0; i < float32Array.length; i++, offset += 2) {
                const s = Math.max(-1, Math.min(1, float32Array[i]));
                view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }

            return new Blob([buffer], { type: "audio/wav" });
        }

        // T·∫£i model ngay khi trang ƒë∆∞·ª£c m·ªü
        loadModel();

    </script>
</body>
</html>
