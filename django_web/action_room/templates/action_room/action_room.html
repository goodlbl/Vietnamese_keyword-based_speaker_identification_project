{% load static %}
<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Wake-Word Detector</title>
    <script src="https://cdn.tailwindcss.com"></script>
    
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.18.0/dist/ort.min.js"></script>
    
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.20.0/dist/tf.min.js"></script>
    
    <style>
        /* Th√™m style cho n√∫t b·∫•m disabled */
        #toggle-btn:disabled {
            background-color: #9ca3af; /* gray-400 */
            cursor: not-allowed;
            animation: none; /* T·∫Øt animation khi disabled */
        }
        
        /* üé® TH√äM M·ªöI: Animation "breathing" cho n√∫t khi ƒëang nghe */
        @keyframes pulse-shadow {
            0%, 100% {
                box-shadow: 0 0 0 0 rgba(59, 130, 246, 0.4); /* blue-500 */
            }
            70% {
                box-shadow: 0 0 0 10px rgba(59, 130, 246, 0);
            }
        }
        #toggle-btn.is-listening {
            /* Khi ch·∫°y, chuy·ªÉn sang m√†u ƒë·ªè v√† gi·ªØ hi·ªáu ·ª©ng */
            background-color: #ef4444; /* red-500 */
            animation: pulse-shadow 2s infinite;
        }
        #toggle-btn.is-listening:hover {
            background-color: #dc2626; /* red-600 */
        }
    </style>
</head>
<body class="bg-gray-100 font-sans antialiased">
    <div class="container mx-auto p-4 sm:p-8 max-w-2xl">
        <div class="bg-white rounded-lg shadow-xl overflow-hidden">
            <div class="p-6 sm:p-8">
                <h1 class="text-2xl sm:text-3xl font-bold text-center text-gray-800 mb-6 flex items-center justify-center">
                    <span class="mr-3">üé§</span> Real-time Wake Word Detector
                </h1>
                
                <p class="text-center text-gray-600 mb-6">
                    Nh·∫•n "B·∫Øt ƒë·∫ßu" v√† th·ª≠ n√≥i t·ª´ kh√≥a.
                    Model ƒëang nghe v√† ph√¢n t√≠ch li√™n t·ª•c trong tr√¨nh duy·ªát.
                </p>

                <div class="flex justify-center mb-6">
                    <button id="toggle-btn"
                            class="bg-blue-500 hover:bg-blue-600 text-white font-semibold py-3 px-6 rounded-lg text-lg transition duration-300"
                            onclick="toggleListening()">
                        üéôÔ∏è B·∫Øt ƒë·∫ßu nghe </button>
                </div>

                <div class="mt-6 min-h-[150px]">
                    <div id="status" class="mt-6 text-center p-4 rounded-lg bg-gray-50 hidden"></div>
                    <div id="features-box" class="mt-6 text-center"></div>
                </div>

                <div class="flex justify-center items-center gap-4 mt-8 border-t pt-6">
                    
                    <a href="{% url 'main_page:home' %}"
                       class="bg-gray-500 hover:bg-gray-600 text-white font-semibold py-3 px-8 rounded-full text-lg shadow-md
                              transform hover:scale-105 transition duration-300 ease-in-out">
                        ‚¨ÖÔ∏è Out
                    </a>

                    <form method="get" action="{% url 'check_password:check_password_view' room.id %}" class="m-0">
                        <button type="submit"
                                class="bg-gradient-to-r from-green-400 to-blue-500 hover:from-green-500 hover:to-blue-600
                                       text-white font-semibold py-3 px-8 rounded-full text-lg shadow-md
                                       transform hover:scale-105 transition duration-300 ease-in-out">
                            üìù ƒêƒÉng k√Ω
                        </button>
                    </form>
                </div>
                </div>
        </div>
    </div>

    <script>
        // --- C√ÄI ƒê·∫∂T C·∫§U H√åNH (GI·ªÆ NGUY√äN) ---
        const ONNX_MODEL_PATH = "{% static 'model/model1/voice_model_from_spec.onnx' %}";
        const ROOM_ID = "{{ room.id }}";
        const TARGET_SR = 16000;
        const TARGET_LENGTH = 40000;
        const N_FFT = 400;
        const HOP_LENGTH = 512;
        const THRESHOLD = 0.5;
        const INFERENCE_INTERVAL_MS = 750;

        // --- Bi·∫øn to√†n c·ª•c (GI·ªÆ NGUY√äN) ---
        let ortSession;
        let isListening = false;
        let audioContext;
        let micStream;
        let scriptProcessor;
        let audioBuffer = new Float32Array(0);
        let inferenceInterval;

        const statusDiv = document.getElementById('status');
        const toggleBtn = document.getElementById('toggle-btn');
        const featuresBox = document.getElementById('features-box');

        // =================================================================
        // C√ÅC H√ÄM C·ªêT L√ïI (GI·ªÆ NGUY√äN)
        // =================================================================
        async function resampleAudio(audioBuffer, targetSampleRate) {
            if (audioBuffer.sampleRate === targetSampleRate) {
                return audioBuffer.getChannelData(0);
            }
            const duration = audioBuffer.duration;
            const offlineContext = new OfflineAudioContext(
                1, Math.ceil(duration * targetSampleRate), targetSampleRate
            );
            const bufferSource = offlineContext.createBufferSource();
            bufferSource.buffer = audioBuffer;
            bufferSource.connect(offlineContext.destination);
            bufferSource.start();
            const resampledBuffer = await offlineContext.startRendering();
            return resampledBuffer.getChannelData(0);
        }

        function padOrCut(data, length) {
            if (data.length > length) {
                return data.slice(0, length);
            } else if (data.length < length) {
                const paddedData = new Float32Array(length).fill(0);
                paddedData.set(data);
                return paddedData;
            }
            return data;
        }

        async function computeSpectrogram(data) {
            const padWidth = N_FFT / 2;
            const inputTensor = tf.tensor1d(data);
            const paddedTensor = tf.mirrorPad(inputTensor, [[padWidth, padWidth]], 'reflect');
            const stft = tf.signal.stft(
                paddedTensor, N_FFT, HOP_LENGTH, N_FFT, tf.signal.hannWindow
            );
            const powerSpec = tf.square(tf.abs(stft));
            const transposedSpec = powerSpec.transpose([1, 0]);
            inputTensor.dispose();
            paddedTensor.dispose();
            stft.dispose();
            powerSpec.dispose();
            return transposedSpec;
        }

        async function runOnnxModel(spectrogramTensor) {
            const specData = await spectrogramTensor.data();
            const dims = spectrogramTensor.shape;
            const ortTensor = new ort.Tensor('float32', specData, [1, ...dims]);
            const inputs = { 'spectrogram': ortTensor };
            const results = await ortSession.run(inputs);
            const logit = results.logits.data[0];
            const probability = 1 / (1 + Math.exp(-logit));
            const prediction = probability > THRESHOLD ? "True" : "False";
            spectrogramTensor.dispose();
            return [prediction, probability];
        }

        // üé® H√ÄM UPDATE UI (GI·ªÆ NGUY√äN)
        function updateStatus(message, type) {
            featuresBox.innerHTML = "";
            statusDiv.classList.remove('hidden', 'bg-gray-50', 'bg-blue-100', 'bg-green-100', 'bg-red-100', 'text-gray-800', 'text-blue-800', 'text-green-800', 'text-red-800');
            let baseClass = 'p-4 rounded-lg text-lg';
            let typeClass = '';
            let icon = '';
            if (type === 'loading') {
                typeClass = 'bg-blue-100 text-blue-800';
                icon = `<svg class="animate-spin h-5 w-5 mr-3" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path></svg>`;
            } else if (type === 'success') {
                typeClass = 'bg-green-100 text-green-800';
                icon = `<svg class="h-6 w-6 mr-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z" /></svg>`;
            } else if (type === 'error') {
                typeClass = 'bg-red-100 text-red-800';
                icon = `<svg class="h-6 w-6 mr-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 14l2-2m0 0l2-2m-2 2l-2-2m2 2l2 2m7-2a9 9 0 11-18 0 9 9 0 0118 0z" /></svg>`;
            } else { // 'info' ho·∫∑c m·∫∑c ƒë·ªãnh
                typeClass = 'bg-gray-100 text-gray-800';
                icon = `<svg class="h-6 w-6 mr-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" /></svg>`;
            }
            statusDiv.className = `${baseClass} ${typeClass}`;
            statusDiv.innerHTML = `<div class="flex items-center justify-center">${icon} <span>${message}</span></div>`;
        }

        // =================================================================
        // LOGIC M·ªöI (REAL-TIME) - (GI·ªÆ NGUY√äN)
        // =================================================================

        async function loadModel() {
            toggleBtn.disabled = true;
            try {
                updateStatus("ƒêang t·∫£i model ONNX...", "loading");
                ortSession = await ort.InferenceSession.create(ONNX_MODEL_PATH);
                updateStatus("Model ƒë√£ s·∫µn s√†ng. Nh·∫•n 'B·∫Øt ƒë·∫ßu nghe' ƒë·ªÉ ch·∫°y.", "info");
                toggleBtn.disabled = false;
            } catch (e) {
                updateStatus(`L·ªói khi t·∫£i model: ${e.message}`, "error");
            }
        }
        
        function toggleListening() {
            if (isListening) {
                stopListening();
            } else {
                startListening();
            }
        }

        async function startListening() {
            if (!ortSession) {
                updateStatus("L·ªói: Model ch∆∞a ƒë∆∞·ª£c t·∫£i.", "error");
                return;
            }
            try {
                updateStatus("Y√™u c·∫ßu quy·ªÅn truy c·∫≠p micro...", "loading");
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                scriptProcessor.onaudioprocess = processAudioChunk;
                const source = audioContext.createMediaStreamSource(micStream);
                source.connect(scriptProcessor);
                scriptProcessor.connect(audioContext.destination);
                inferenceInterval = setInterval(runInference, INFERENCE_INTERVAL_MS);
                isListening = true;
                toggleBtn.textContent = "‚èπÔ∏è D·ª´ng nghe";
                toggleBtn.classList.add('is-listening');
                updateStatus("ƒêang nghe...", "loading");
            } catch (e) {
                updateStatus(`L·ªói micro: ${e.message}. Vui l√≤ng c·∫•p quy·ªÅn truy c·∫≠p.`, "error");
            }
        }

        function stopListening() {
            if (!isListening) return;
            try {
                clearInterval(inferenceInterval);
                inferenceInterval = null;
                scriptProcessor.disconnect();
                micStream.getTracks().forEach(track => track.stop());
                audioContext.close();
                isListening = false;
                audioBuffer = new Float32Array(0);
                toggleBtn.textContent = "üéôÔ∏è B·∫Øt ƒë·∫ßu nghe";
                toggleBtn.classList.remove('is-listening');
                updateStatus("ƒê√£ d·ª´ng. Nh·∫•n 'B·∫Øt ƒë·∫ßu' ƒë·ªÉ nghe l·∫°i.", "info");
            } catch (e) {
                console.error("L·ªói khi d·ª´ng:", e);
                updateStatus("ƒê√£ d·ª´ng.", "info");
            }
        }

        function processAudioChunk(event) {
            if (!isListening) return;
            const newChunk = event.inputBuffer.getChannelData(0);
            const newBuffer = new Float32Array(audioBuffer.length + newChunk.length);
            newBuffer.set(audioBuffer);
            newBuffer.set(newChunk, audioBuffer.length);
            audioBuffer = newBuffer;
            const maxBufferLength = (audioContext.sampleRate * 5); 
            if (audioBuffer.length > maxBufferLength) {
                audioBuffer = audioBuffer.slice(audioBuffer.length - maxBufferLength);
            }
        }

        async function runInference() {
            if (!isListening) return;
            const requiredNativeSamples = Math.floor(TARGET_LENGTH * (audioContext.sampleRate / TARGET_SR));
            if (audioBuffer.length < requiredNativeSamples) {
                updateStatus("ƒêang nghe (ch·ªù ƒë·ªß buffer)...", "loading");
                return;
            }
            const audioChunk = audioBuffer.slice(audioBuffer.length - requiredNativeSamples);
            toggleBtn.disabled = true;
            updateStatus("ƒêang ph√¢n t√≠ch...", "loading");

            try {
                const bufferToResample = audioContext.createBuffer(1, audioChunk.length, audioContext.sampleRate);
                bufferToResample.copyToChannel(audioChunk, 0);
                const resampledData = await resampleAudio(bufferToResample, TARGET_SR);
                const processedData = padOrCut(resampledData, TARGET_LENGTH);
                const spectrogramTensor = await computeSpectrogram(processedData);
                const [prediction, probability] = await runOnnxModel(spectrogramTensor);

                if (prediction === "True" && probability > THRESHOLD) {
                    updateStatus(
                        `<strong>Ph√°t hi·ªán t·ª´ kh√≥a!</strong> (X√°c su·∫•t: ${(probability * 100).toFixed(2)}%)<br>
                         üîÑ ƒêang g·ª≠i t·ªõi Model2 (x√°c th·ª±c ng∆∞·ªùi n√≥i)...`,
                        "loading"
                    );
                    stopListening();
                    try {
                        const wavBlob = await exportWavBlob(audioChunk, audioContext.sampleRate);
                        const formData = new FormData();
                        formData.append("audio", wavBlob, "captured.wav");
                        formData.append("room_id", ROOM_ID);
                        
                        const response = await fetch("/action_room/verify_voice/", {
                            method: "POST",
                            body: formData
                        });
                        const data = await response.json();
                        if (data.error) {
                            updateStatus(`‚ùå L·ªói: ${data.error}`, "error");
                        } else {
                            const resultMessage = `
                                üé§ Gi·ªçng ng∆∞·ªùi n√≥i: <strong>${data.owner}</strong><br>
                                üîπ Similarity: <strong>${(data.similarity).toFixed(2)}</strong>
                                üîπ K·∫øt qu·∫£: <strong>${data.is_match ? "‚úÖ Kh·ªõp" : "‚ùå Kh√¥ng kh·ªõp"}</strong>
                                <div class="mt-3 text-sm font-medium text-gray-700">
                                    Ho√†n t·∫•t. B·∫°n c√≥ th·ªÉ b·∫•m 'B·∫Øt ƒë·∫ßu nghe' ƒë·ªÉ ti·∫øp t·ª•c.
                                </div>
                            `;
                            updateStatus(resultMessage, data.is_match ? "success" : "error");
                            if (data.is_match) {
                                if (data.functions && data.functions.length > 0) {
                                    const colors = ["bg-red-500", "bg-green-500", "bg-blue-500", "bg-pink-500", "bg-orange-500", "bg-teal-500"];
                                    const html = data.functions.map((f, i) =>
                                        `<button class="${colors[i % colors.length]} text-white px-4 py-2 rounded-lg m-2 hover:opacity-90">${f}</button>`
                                    ).join("");
                                    featuresBox.innerHTML = `
                                        <div class="mt-4 bg-gray-50 p-4 rounded-xl shadow-inner">
                                            <h3 class="text-lg font-bold mb-2 text-gray-700">Thi·∫øt b·ªã c·ªßa ${data.owner}</h3>
                                            ${html}
                                        </div>
                                    `;
                                } else {
                                    featuresBox.innerHTML = `<p class="text-gray-500 mt-4 italic">Kh√¥ng c√≥ thi·∫øt b·ªã n√†o ƒë∆∞·ª£c ph√¢n quy·ªÅn.</p>`;
                                }
                            }
                        }
                        toggleBtn.disabled = false;
                        toggleBtn.textContent = "üéôÔ∏è B·∫Øt ƒë·∫ßu nghe";
                        toggleBtn.classList.remove('is-listening');
                    } catch (err) {
                        console.error("L·ªói khi g·ª≠i Django/Flask:", err);
                        updateStatus(`üî• L·ªói khi x√°c th·ª±c gi·ªçng n√≥i: ${err.message}`, "error");
                    }
                } else {
                    updateStatus(
                        `ƒêang nghe... (G·∫ßn nh·∫•t: ${(probability * 100).toFixed(2)}%)`,
                        "loading"
                    );
                    toggleBtn.disabled = false;
                }
            } catch (e) {
                updateStatus(`L·ªói khi d·ª± ƒëo√°n: ${e.message}`, "error");
                toggleBtn.disabled = false;
            }
        }

        // üîß Chuy·ªÉn m·∫£ng Float32Array th√†nh file WAV (GI·ªÆ NGUY√äN)
        async function exportWavBlob(float32Array, sampleRate) {
            const buffer = new ArrayBuffer(44 + float32Array.length * 2);
            const view = new DataView(buffer);
            const writeString = (offset, str) => {
                for (let i = 0; i < str.length; i++) view.setUint8(offset + i, str.charCodeAt(i));
            };
            writeString(0, "RIFF");
            view.setUint32(4, 36 + float32Array.length * 2, true);
            writeString(8, "WAVE");
            writeString(12, "fmt ");
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, 1, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(36, "data");
            view.setUint32(40, float32Array.length * 2, true);
            let offset = 44;
            for (let i = 0; i < float32Array.length; i++, offset += 2) {
                const s = Math.max(-1, Math.min(1, float32Array[i]));
                view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }
            return new Blob([buffer], { type: "audio/wav" });
        }

        // T·∫£i model ngay khi trang ƒë∆∞·ª£c m·ªü
        loadModel();

    </script>
</body>
</html>